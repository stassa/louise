:-module(path, [background_knowledge/2
	       ,metarules/2
	       ,positive_example/2
	       ,negative_example/2
	       ,edge/2
	       ,not_edge/2
	       ]).

/** <module> Experiment file demonstrating examples invention.

TODO: Update to latest version of Louise (Vanilla).

Configuration options
---------------------

The experiments described in this file were carried out with the
following configuration options. Important options are marked with an
asterisk (*):

==
?- list_config.
* clause_limit(0)
example_clauses(call)
* experiment_file(data/examples/example_invention.pl,path)
fold_recursive(false)
generalise_learned_metarules(false)
learner(louise)
* learning_predicate(learn_with_examples_invention/2)
listing_limit(10)
* max_error(0,0)
* max_invented(0)
metarule_formatting(quantified)
metarule_learning_limits(none)
minimal_program_size(2,inf)
recursive_reduction(false)
reduce_learned_metarules(false)
* reduction(plotkins)
* resolutions(5000)
theorem_prover(resolution)
unfold_invented(false)
true.
==


Learning problem
----------------

Ensure the listing of the elements of the MIL problem looks like the
lising below:

==
?- list_mil_problem(path/2).
Positive examples
-----------------
path(a,f).

Negative examples
-----------------
[]

Background knowledge
--------------------
edge/2:
edge(a,b).
edge(b,c).
edge(c,d).
edge(d,e).
edge(e,f).

not_edge/2:
not_edge(1,2).
not_edge(1,3).
not_edge(1,4).

Metarules
---------
(Chain) ∃.P,Q,R ∀.x,y,z: P(x,y)← Q(x,z),R(z,y)
(Identity) ∃.P,Q ∀.x,y: P(x,y)← Q(x,y)

Metasubstitution constraints
----------------------------
:- dynamic configuration:metarule_constraints/2.
:- multifile configuration:metarule_constraints/2.

configuration:metarule_constraints(M, fail) :-
    path:(M=..[m, _Id, P, P|_Ps]).

true.
==


Examples invention
------------------

The MIL problem defined in the current experiment file can't be solved
with the configuration option clause_limit(1). That's because setting
clause_limit(1) forces Louise's Top Program Construction algorithm (TPC)
to construct only one instance of each metarule at a time and neither of
the metarules _Chain_ and _Identity_ is sufficient to express a theory
entailing the single example path(a,f) given the background definition
of edge/2 in this experiment file.

In particular, the _Chain_ metarule, having only two body literals,
cannot represent a path (through the graph defined by edge/2 clauses in
the BK) from vertex a to vertex f, because those vertices are separated
by a distance of five edges and each literal of the _Chain_ metarule can
only represent a single edge. A first learning attempt therefore fails
to learn a general hypothesis and simply returns the single example:

==
?- learn(path/2).
path(a,f).
true.
==

With examples invention, atoms of path/2 represenging shorter paths
between nodes are derived from the background definition of edge/2 and
_Chain_ and _Identity_. These shorter paths are short enough that they
can be represented by instances of _Chain_ and _Identity_.

Adding these invented examples to the set of positive examples allows
Louise to learn a hypothesis general enough to entail not just all
invented examples, but also the initial, path(a,f), example, even with
clause_limit(1).

The following query demonstrate how examples invention can be used to
solve the MIL problem defined in this experiment file. Note that the
debugging output of the query has been edited for improved readability
and with sections numbered for later reference:

==
?- debug(examples_invention).
true.

% Learning with invented positive examples.
?- learn_with_examples_invention(path/2, _Ps), print_clauses(_Ps).
% Inventing examples
% Partial examples
% path(A,f)
% path(a,A)

% Learning with partial examples...        [1]
% Top Program for partial examples:
% m(path,A,B):-m(edge,A,B)
% m(path,A,B):-m(edge,A,C),m(edge,C,B)
% m(path,A,B):-m(edge,A,C),m(path,C,B)
% m(path,A,B):-m(not_edge,A,C),m(path,C,B)

% Given and invented examples              [2]
% path(a,b)
% path(a,c)
% path(a,d)
% path(a,e)
% path(a,f)
% path(b,c)
% path(b,d)
% path(b,e)
% path(b,f)
% path(c,d)
% path(c,e)
% path(c,f)
% path(d,e)
% path(d,f)
% path(e,f)

% Encapsulating problem                   [3]
% Constructing Top program...
% Reducing Top program
% Excapsulating hypothesis
path(A,B):-edge(A,B).
path(A,B):-edge(A,C),path(C,B).
true.
==

In the following sections we further discuss the output of the above
query to learn_with_examples_invention/2.


Generalisation of partial examples
----------------------------------

The first three lines in the debug output of
learn_with_examples_invention/2, above, are generated by
examples_invention/5 (the predicate that performs examples invention):

==
% Learning with partial examples...        [1]
% Top Program for partial examples:
% m(path,A,B):-m(edge,A,B)
% m(path,A,B):-m(edge,A,C),m(edge,C,B)
% m(path,A,B):-m(edge,A,C),m(path,C,B)
% m(path,A,B):-m(not_edge,A,C),m(path,C,B)
==

The clauses listed in [1] are generalisations of the single positive
example, path(a,f) derived from the _Chain_ and _Identity_ metarules and
the background definition of edge/2.

The clauses in [1] are obtained by Top program construction with the
partially instantiated examples path(a,X) and path(Y,f) added to the
given MIL problem (i.e. the positive and negative examples, BK and
metarules specified in this experiment file). These partial examples are
derived from the given example, path(a,f), by successively replacing
each of its constants with a variable. Replacing the constants in the
path(a,f) example with variables results in atoms of path/2 representing
sub-paths, on the graph represented by edge/2, of the path from "a" to
"f".

The first partial example, path(a,X) represents all paths that can be
reached from the node "a". The second partial example, path(Y,f)
represents all paths that can reach the node "f". Their generalisation
is a set of clauses that suffice to represent all paths from node "a" to
node "f".

Note that, because in this case we have a single example and the
background definition of edge/2 makes for a very simple graph (a
straight path from "a" to "f"), partial examples only represent
sub-paths of path(a,f). In a larger problem, partial examples may also
reprsent paths encompassing path(a,f) as well as paths disjoint to
path(a,f) (e.g. paths starting from node "a" and not crossing the path
from "a" to "f" etc).


Invented examples
-----------------

The next few debug lines list the union of the set of positive
examples, including the one initially given positive example, path(a,f),
and the Least Herbrand Model of the two clauses in [1], i.e. the clauses
derived from the generalisation of the partial examples described in the
previous section:

==
% Given and invented examples              [2]
% path(a,b)
% path(a,c)
% path(a,d)
% path(a,e)
% path(a,f) % Given example
% path(b,c)
% path(b,d)
% path(b,e)
% path(b,f)
% path(c,d)
% path(c,e)
% path(c,f)
% path(d,e)
% path(d,f)
% path(e,f)
==

The given example, path(a,f) is marked in the debug output block above.
Remaining atoms in the debug output block are invented examples.

The Least Herbrand Model of the two clauses in [1] is generated by
bottom-up evaluation (performed by lfp_query/4, defined in lib(tp/tp)).

The atoms of path/2 in the LHM of the two clauses in [1] are a set of
paths connecting nodes that can be reached in at most two steps. That is
the maximum number of steps in a path that can be represented by
instances of the _Chain_ and _Identity_	metarules alone.

Note that, while the two clauses generalising partial examples, in [1],
were derived from partial examples representing all paths from node "a"
or to node "f", the invented positive examples in [2] include paths
staring or ending at nodes other than "a" and "f", e.g. path(b,c),
path(d,e), etc. However, each of those sub-paths are paths "on the way"
to node "f" from node "a".


Lerning with invented examples
------------------------------

The entire set of invented examples in [2] includes sub-paths of the
path represented by the single example path(a,f):

==
path(a,b), path(b,c), path(c,d), path(d,e), path(e,f)
==

Each of these sub-paths is short enough to be represented by instances
of the _Chain_ and _Identity_ metarules and the background knowledge
definitions of edge/2. All of those sub-paths taken together suffice to
represent the full path from "a" to "f" using instances of _Chain_ and
_Inverse_.

The result is that with the invented examples in [2] we now have
sufficient examples, background knowledge and metarules to learn a
hypothesis that entails the positive example path(a,f), listed in [3]:

==
% Encapsulating problem                   [3]
% Constructing Top program...
% Reducing Top program
% Excapsulating hypothesis
path(A,B):-edge(A,B).
path(A,B):-edge(A,C),path(C,B).
true.
==

Note that the learned hypothesis does not include the two clauses in
[1], i.e. the generalisations of the partial examples. That is because
the learned hypothesis is learned with only fully-instantiated examples,
the given example, path(a,f) and the invented examples in [2].

The end result is that the learned hypothesis is a hypothesis general
enough that it entails all ground positive examples, invented, or given,
but specific enough that it does not entail the over-general, partial
instantiations of path(a,f) listed in [2].


Irrelevant background knowledge
-------------------------------

The predicates specified as background knowledge for path/2 include a
definition of a predicate not_edge/2:

==
not_edge(1,2).
not_edge(1,3).
not_edge(1,4).
==

not_edge/2 is an irrelevant background predicate- a "decoy", meant to
test the extent to which examples invention results in spurious examples
(i.e. false positives) when given irrelevant background knowledge.

The listing of invented examples in [2] make it clear that no false
positives were invented in this case:

==
% Given and invented examples              [2]
% path(a,b)
% path(a,c)
% path(a,d)
% path(a,e)
% path(a,f) % Given example
% path(b,c)
% path(b,d)
% path(b,e)
% path(b,f)
% path(c,d)
% path(c,e)
% path(c,f)
% path(d,e)
% path(d,f)
% path(e,f)
==

For instance, a false positive derived from not_edge/2 might include
path(1,b). Such false positives are excluded by the partial examples.
Partial examples are partially instantiated to the constants in a
true example, so atoms of background knowledge with a domain disjoint to
the domain of the positive examples cannot unify with partial examples,
and will not end up in the set of invented examples.

The situation is different with background knowledge that is irrelevant
but whose domain may partially overlap with the positive examples. For
instance, if we were to redefine not_edge/2 as follows:

==
not_edge(1,b).
not_edge(a,3).
not_edge(a,4).
==

Then the results of example invention would include false positives and
the learned hypothesis would be over-general:

==
?- learn_with_examples_invention(path/2).
% Inventing examples
% Partial examples
% path(A,f)
% path(a,A)
% Learning with partial examples...

% Top Program for partial examples:
% m(path,A,B):-m(edge,A,B)
% m(path,A,B):-m(not_edge,A,B)
% m(path,A,B):-m(edge,A,C),m(edge,C,B)
% m(path,A,B):-m(edge,A,C),m(path,C,B)
% m(path,A,B):-m(not_edge,A,C),m(path,C,B)

% Given and invented examples
% path(1,b)
% path(1,c)
% path(1,d)
% path(1,e)
% path(1,f)
% path(a,3)
% path(a,4)
% path(a,b)
% path(a,c)
% path(a,d)
% path(a,e)
% path(a,f)
% path(b,c)
% path(b,d)
% path(b,e)
% path(b,f)
% path(c,d)
% path(c,e)
% path(c,f)
% path(d,e)
% path(d,f)
% path(e,f)

% Encapsulating problem
% Constructing Top program...
% Reducing Top program
% Excapsulating hypothesis
path(A,B):-edge(A,B).
path(A,B):-not_edge(A,B).
path(A,B):-edge(A,C),path(C,B).
path(A,B):-not_edge(A,C),path(C,B).
true.
==

The need to select only relevant background knowledge is a limitation of
examples invention. Some irrelevant backround predicates may result in
false positives and over-generalising hypotheses.


Learning with looser clause limits
----------------------------------

A correct hypothesis of path/2 can be learned from the elements of the
MIL problem, with clause_limit(2):

==
?- list_config.
clause_limit(2)
% ... more options

?- learn(path/2).
path(A,B):-edge(A,B).
path(A,B):-edge(A,C),path(C,B).
true.
==

Examples invention is an alternative way to learn from a single example.

*/

% Constraint excluding left-recursive hypotheses.
% Also has the effect of excluding generalisations of partial
% positive examples such as the following from the Top Program learned
% with invented examples:
% path(A,B):-edge(A,C),edge(C,B).
% path(A,B):-path(A,C),path(C,B).
%
configuration:metarule_constraints(M,fail):-
	M =.. [m,_Id,P,P|_Ps].

% Tells list_learning_results/0 to use the right learning predicate.
configuration:learning_predicate(learn_with_examples_invention/2).

background_knowledge(path/2, [edge/2,not_edge/2]).

metarules(path/2,[chain,identity]).
% Alternative metarules. Try them out.
%metarules(path/2,[chain,identity,inverse,switch]).

positive_example(path/2,path(a,f)).

negative_example(path/2,_):-
	fail.

edge(a,b).
edge(b,c).
edge(c,d).
edge(d,e).
edge(e,f).


%/* Irrelevant BK
not_edge(1,2).
not_edge(1,3).
not_edge(1,4).
%*/

/* Partially relevant BK
not_edge(1,b).
not_edge(a,3).
not_edge(a,4).
*/

/*
Target theory:

path(X,Y):-
	edge(X,Y).
path(X,Y):-
	edge(X,Z)
	,path(Z,Y).
*/
